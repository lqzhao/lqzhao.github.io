<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Linqing Zhao 赵林清</title>
 
    <meta name="author" content="Linqing Zhao">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <style>
  .justify-text { text-align: justify; width: 400px; /* 或者其他宽度 */}
</style>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle;text-align: justify">
                <p class="name" style="text-align: center;">
                  Linqing Zhao（赵林清）
                </p>
                <p>
                  I'm currently a Postdoctoral fellow of <a href="https://www.au.tsinghua.edu.cn/en/index.htm">Department of Automation</a>, <a href="https://www.tsinghua.edu.cn/en/">Tsinghua University</a>, affiliated with <a href="http://ivg.au.tsinghua.edu.cn/">Intelligent Vision Group (IVG)</a>, supervised by <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Prof. Jiwen Lu</a>. 
                <br> 
                <br> 
                  I received the B.Eng. and Ph.D. degrees in information and communication engineering from the <a href="https://seea.tju.edu.cn/">School of Electrical and Information Engineering</a>, <a href="https://www.tju.edu.cn/">Tianjin University</a>, China, in 2017 and 2024, respectively, supervised by Prof. Zhanjie Song. During my PhD studies, I was honored to be a visiting student at <a href="http://ivg.au.tsinghua.edu.cn/">Intelligent Vision Group (IVG)</a>, supervised by <a href="https://www.au.tsinghua.edu.cn/info/1152/3127.htm">Prof. Jie Zhou</a> and <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Prof. Jiwen Lu</a>.
                <br> 
                <br> 
                  My research interests lie in computer vision, especially robot vision, autonomous driving perception, and deep learning.
                </p>
                <p style="text-align:center">
                <a href="mailto:zhaolinqing@mail.tsinghua.edu.cn">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=ypxt5UEAAAAJ&hl"> Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/lqzhao"> Github </a> &nbsp/&nbsp
		<a href="images/个人简历.pdf"> 中文版简历 </a>
              </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/linqingzhao.jpg"><img style="width:80%;max-width:80%;object-fit: cover; border-radius: 10%;" alt="profile photo" src="images/linqingzhao.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
       </tbody></table>

          
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <h2>News</h2>
            <p>
              <li style="margin: 5px;" >
                <b>2025-06:</b> Two papers on RGB SLAM and Camera Pose Estimation, and one co-author paper on Indoor 3D Object Detection are accepted to <a href="https://www.iros25.org/">IROS 2025</a>.
              <li style="margin: 5px;" >
                <b>2025-05:</b> Achieved an 'Excellent' rating in the postdoctoral midterm assessment, ranking top 3 of 24.
              </li>
              <li style="margin: 5px;" >
                <b>2025-04:</b> One co-author paper on 3D Object Detection is accepted to <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=76">TCSVT</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2025-04:</b> One paper on Multi-modal Semantic Segmentation is accepted to <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6046">TMM</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2025-03:</b> One co-author paper on Zero-shot Object Navigation is accepted to <a href="https://cvpr.thecvf.com/Conferences/2025">CVPR 2025</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2025-01:</b> One co-author paper on Online 3D Instance Segmentation is accepted to <a href="https://iclr.cc/Conferences/2025">ICLR 2025</a> (oral paper).
              </li>
              <li style="margin: 5px;" >
                <b>2024-12:</b> Selected into <a href="https://postdoctor.tsinghua.edu.cn/thu/index.htm">Shuimu Tsinghua Scholar Program</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2024-05:</b> One paper on Lane Detection is accepted to <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">TIP</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2024-02:</b> Two papers on 3D Occupancy and Online 3D Scene Perception are accepted to <a href="https://cvpr.thecvf.com/Conferences/2024">CVPR 2024</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2024-01:</b> One paper on Depth Completion is accepted to <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">TIP</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2023-09:</b> One paper on Unsupervised Depth Completion is accepted to <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=76">TCSVT</a>.
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>

          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle;text-align: justify">
                <h2>Recent Selected Publications</h2>
                <p>
                  (*Equal Contribution, #Corresponding Author)
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

		  
                <tr onmouseout="LowRankOcc_stop()" onmouseover="LowRankOcc_start()">
      <td style="padding:20px;width:25%;vertical-align:top;text-align: justify">
        <div class="one">
          <div class="two" id='LowRankOcc'><video  width=100% muted autoplay loop>
          <source src="images/Pseudo_Depth.png" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/Pseudo_Depth.png' width=110%>
        </div>
        <script type="text/javascript">
          function LowRankOcc_start() {
            document.getElementById('LowRankOcc').style.opacity = "1";
          }

          function LowRankOcc_stop() {
            document.getElementById('LowRankOcc').style.opacity = "0";
          }
          LowRankOcc_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:top;text-align: justify">
        <span class="papertitle">Pseudo Depth Meets Gaussian: A Feed-forward RGB SLAM Baseline </span>  
        <br>      
        <span class="author">
              <strong>Linqing Zhao</strong>*, <a href="https://xuxw98.github.io/"> Xiuwei Xu</a>*, <a href="https://github.com/wangyr22/"> Yirui Wang</a>, Hao Wang, <a href="https://wzzheng.net/"> Wenzhao Zheng</a>, 
		<a href="https://andytang15.github.io/"> Yansong Tang</a>, Haibin Yan#,
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu</a>
        <br> 
        <span class="publications"><em>IEEE/RSJ International Conference on Intelligent Robots and Systems (<strong>IROS</strong>), 2025. </em></span>
        <br> 
        <p>
          We propose an online 3D reconstruction method that utilizes only monocular RGB input, eliminating the need for depth sensors or expensive iterative pose optimization.
        </p> 
      </td>
    </tr>

                <tr onmouseout="LowRankOcc_stop()" onmouseover="LowRankOcc_start()">
      <td style="padding:20px;width:25%;vertical-align:top;text-align: justify">
        <div class="one">
          <div class="two" id='LowRankOcc'><video  width=100% muted autoplay loop>
          <source src="images/iGaussian.png" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/iGaussian.png' width=110%>
        </div>
        <script type="text/javascript">
          function LowRankOcc_start() {
            document.getElementById('LowRankOcc').style.opacity = "1";
          }

          function LowRankOcc_stop() {
            document.getElementById('LowRankOcc').style.opacity = "0";
          }
          LowRankOcc_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:top;text-align: justify">
        <span class="papertitle">iGaussian: Real-Time Camera Pose Estimation via Feed-Forward 3D Gaussian Splatting Inversion </span>  
        <br>      
        <span class="author">
              Hao Wang*, <strong>Linqing Zhao</strong>*, <a href="https://xuxw98.github.io/"> Xiuwei Xu</a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu</a>, Haibin Yan#
              
        <br> 
        <span class="publications"><em>IEEE/RSJ International Conference on Intelligent Robots and Systems (<strong>IROS</strong>), 2025. </em></span>
        <br> 
        <p>
           We propose iGaussian, a two-stage feed-forward framework that achieves real-time camera pose estimation through direct 3D Gaussian inversion.
        </p> 
      </td>
    </tr>

                <tr onmouseout="LowRankOcc_stop()" onmouseover="LowRankOcc_start()">
      <td style="padding:20px;width:25%;vertical-align:top;text-align: justify">
        <div class="one">
          <div class="two" id='LowRankOcc'><video  width=100% muted autoplay loop>
          <source src="images/HSA.png" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/SAFNet.png' width=110%>
        </div>
        <script type="text/javascript">
          function LowRankOcc_start() {
            document.getElementById('LowRankOcc').style.opacity = "1";
          }

          function LowRankOcc_stop() {
            document.getElementById('LowRankOcc').style.opacity = "0";
          }
          LowRankOcc_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:top;text-align: justify">
        <span class="papertitle">Similarity-Aware Fusion Network for Robust Multi-Modal Semantic Segmentation </span>  
        <br>      
        <span class="author">
              <strong>Linqing Zhao</strong>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu</a>#,
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou</a>
        <br> 
	<span class="publications"><em>IEEE Transactions on Multimedia (<strong>TMM</strong>), 2025. </em></span>
	<br> 
        <span class="publications"><em>IEEE/RSJ International Conference on Intelligent Robots and Systems (<strong>IROS</strong>), 2021. </em></span>
        <br> 
        <p>
          We propose a similarity-aware fusion network (SAFNet) to adaptively fuse 2D images and 3D point clouds for Multi-Modal semantic segmentation.
        </p> 
      </td>
    </tr>


    <tr onmouseout="StructLane_stop()" onmouseover="StructLane_start()">
      <td style="padding:20px;width:25%;vertical-align:top;text-align: justify">
        <div class="one">
          <div class="two" id='StructLane'><video  width=100% muted autoplay loop>
          <source src="images/StructLane.png" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/StructLane.png' width=110%>
        </div>
        <script type="text/javascript">
          function StructLane_start() {
            document.getElementById('StructLane').style.opacity = "1";
          }

          function StructLane_stop() {
            document.getElementById('StructLane').style.opacity = "0";
          }
          StructLane_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:top;text-align: justify">
        <span class="papertitle">StructLane: Leveraging Structural Relations for Lane Detection </span>  
        <br>      
        <span class="author"><strong>Linqing Zhao</strong>, <a href="https://wzzheng.net/"> Wenzhao Zheng</a>, <a href="https://scholar.google.com/citations?user=UgadGL8AAAAJ&hl=zh-CN&oi=ao"> Yunpeng Zhang</a>, <a href="https://www.au.tsinghua.edu.cn/info/1152/3127.htm">Jie Zhou</a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a># </span>       
        <br> 
        <span class="publications"><em>IEEE Transactions on Image Processing (<strong>TIP</strong>), 2024. </em></span>
        <br> 
        <p>
          We propose the StructLane method to enhance lane detection accuracy and robustness by harnessing the structural relationships among lanes.
        </p> 
      </td>
    </tr>

                <tr onmouseout="LowRankOcc_stop()" onmouseover="LowRankOcc_start()">
      <td style="padding:20px;width:25%;vertical-align:top;text-align: justify">
        <div class="one">
          <div class="two" id='LowRankOcc'><video  width=100% muted autoplay loop>
          <source src="images/LowRankOcc.png" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/LowRankOcc.png' width=110%>
        </div>
        <script type="text/javascript">
          function LowRankOcc_start() {
            document.getElementById('LowRankOcc').style.opacity = "1";
          }

          function LowRankOcc_stop() {
            document.getElementById('LowRankOcc').style.opacity = "0";
          }
          LowRankOcc_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:top;text-align: justify">
        <span class="papertitle">LowRankOcc: Tensor Decomposition and Low-Rank Recovery for Vision-based 3D Semantic Occupancy Prediction </span>  
        <br>      
        <span class="author"><strong>Linqing Zhao</strong>, <a href="https://xuxw98.github.io/"> Xiuwei Xu</a>, <a href="https://ziweiwangthu.github.io/"> Ziwei Wang</a>, <a href="https://scholar.google.com/citations?user=UgadGL8AAAAJ&hl=zh-CN&oi=ao"> Yunpeng Zhang</a>, <a href="https://boruizhang.site/"> Borui Zhang</a>, <a href="https://wzzheng.net/"> Wenzhao Zheng</a>, Dalong Du, <a href="https://www.au.tsinghua.edu.cn/info/1152/3127.htm">Jie Zhou</a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a># </span>       
        <br> 
        <span class="publications"><em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2024. </em></span>
        <br> 
        <p>
          We propose LowRankOcc to address spatial redundancy in 3D semantic occupancy prediction, leveraging the inherent low-rank property of occupancy data.
        </p> 
      </td>
    </tr>


                <tr onmouseout="LowRankOcc_stop()" onmouseover="LowRankOcc_start()">
      <td style="padding:20px;width:25%;vertical-align:top;text-align: justify">
        <div class="one">
          <div class="two" id='LowRankOcc'><video  width=100% muted autoplay loop>
          <source src="images/SCMT.png" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/SCMT.png' width=110%>
        </div>
        <script type="text/javascript">
          function LowRankOcc_start() {
            document.getElementById('LowRankOcc').style.opacity = "1";
          }

          function LowRankOcc_stop() {
            document.getElementById('LowRankOcc').style.opacity = "0";
          }
          LowRankOcc_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:top;text-align: justify">
        <span class="papertitle">Structure-aware Cross-Modal Transformer for Depth Completion </span>  
        <br>      
        <span class="author"><strong>Linqing Zhao</strong>, <a href="https://weiyithu.github.io/">Yi Wei</a>, <a href="https://www.jiaxinli.me/">Jiaxin Li</a>, <a href="https://www.au.tsinghua.edu.cn/info/1152/3127.htm">Jie Zhou</a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a># </span>       
        <br> 
        <span class="publications"><em>IEEE Transactions on Image Processing (<strong>TIP</strong>), 2024. </em></span>
        <br> 
        <p>
          We disentangle the hierarchical 3D scene-level structure from the RGB-D input and construct a pathway to make sharp depth boundaries and object shape outlines accessible to 2D features.
        </p> 
      </td>
    </tr>

                <tr onmouseout="LowRankOcc_stop()" onmouseover="LowRankOcc_start()">
      <td style="padding:20px;width:25%;vertical-align:top;text-align: justify">
        <div class="one">
          <div class="two" id='LowRankOcc'><video  width=100% muted autoplay loop>
          <source src="images/SCMT.png" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/SPTR.png' width=110%>
        </div>
        <script type="text/javascript">
          function LowRankOcc_start() {
            document.getElementById('LowRankOcc').style.opacity = "1";
          }

          function LowRankOcc_stop() {
            document.getElementById('LowRankOcc').style.opacity = "0";
          }
          LowRankOcc_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:top;text-align: justify">
        <span class="papertitle">SPTR: Structure-Preserving Transformer for Unsupervised Indoor Depth Completion </span>  
        <br>      
        <span class="author"><strong>Linqing Zhao</strong>, <a href="https://wzzheng.net/"> Wenzhao Zheng</a>, <a href="https://duanyueqi.github.io/">Yueqi Duan</a>, <a href="https://www.au.tsinghua.edu.cn/info/1152/3127.htm">Jie Zhou</a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a># </span>       
        <br> 
        <span class="publications"><em>IEEE Transactions on Circuits and Systems for Video Technology (<strong>TCSVT</strong>), 2023. </em></span>
        <br> 
        <p>
          We propose to reformulate depth completion as the process of 3D structure generation, where the generated structure should recover the complete scene and also consist with the known partial structure.
        </p> 
      </td>
    </tr>

                <tr onmouseout="LowRankOcc_stop()" onmouseover="LowRankOcc_start()">
      <td style="padding:20px;width:25%;vertical-align:top;text-align: justify">
        <div class="one">
          <div class="two" id='LowRankOcc'><video  width=100% muted autoplay loop>
          <source src="images/SCMT.png" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/surroundocc.png' width=110%>
        </div>
        <script type="text/javascript">
          function LowRankOcc_start() {
            document.getElementById('LowRankOcc').style.opacity = "1";
          }

          function LowRankOcc_stop() {
            document.getElementById('LowRankOcc').style.opacity = "0";
          }
          LowRankOcc_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:top;text-align: justify">
        <span class="papertitle">SurroundOcc: Multi-Camera 3D Occupancy Prediction for Autonomous Driving </span>  
        <br>      
        <span class="author"><a href="https://weiyithu.github.io/"> Yi Wei*</a>, <strong>Linqing Zhao</strong>*, <a href="https://wzzheng.net/"> Wenzhao Zheng</a>, 
                        <a href="https://scholar.google.com/citations?user=NmwjI0AAAAAJ&hl=zh-CN&oi=sra"> Zheng Zhu</a>, 
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>,
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu</a># </span>       
        <br> 
        <span class="publications"><em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>), 2023 </em></span>
        <br> 
        <p>
          We design a pipeline to generate dense occupancy ground truths without expensive occupancy annotations, which enables the training of more dense 3D occupancy prediction models.
        </p> 
      </td>
    </tr>

                <tr onmouseout="LowRankOcc_stop()" onmouseover="LowRankOcc_start()">
      <td style="padding:20px;width:25%;vertical-align:top;text-align: justify">
        <div class="one">
          <div class="two" id='LowRankOcc'><video  width=100% muted autoplay loop>
          <source src="images/surrounddepth.gif" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/DHPM.gif' width=110%>
        </div>
        <script type="text/javascript">
          function LowRankOcc_start() {
            document.getElementById('LowRankOcc').style.opacity = "1";
          }

          function LowRankOcc_stop() {
            document.getElementById('LowRankOcc').style.opacity = "0";
          }
          LowRankOcc_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:top;text-align: justify">
        <span class="papertitle">Dense Hybrid Proposal Modulation for Lane Detection </span>  
        <br>      
        <span class="author">
              Yuejian Wu, 
              <strong>Linqing Zhao</strong>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu</a>,
              <a href="https://scholar.google.com/citations?user=-AQLKlsAAAAJ&hl=zh-CN&oi=ao"> Haibin Yan</a># </span> 
        <br> 
        <span class="publications"><em>IEEE Transactions on Circuits and Systems for Video Technology (<strong>TCSVT</strong>), 2023. </em></span>
        <br> 
        <p>
          We densely modulate all proposals to generate topologically and spatially high-quality lane predictions with discriminative representations.
        </p> 
      </td>
    </tr>

                <tr onmouseout="LowRankOcc_stop()" onmouseover="LowRankOcc_start()">
      <td style="padding:20px;width:25%;vertical-align:top;text-align: justify">
        <div class="one">
          <div class="two" id='LowRankOcc'><video  width=100% muted autoplay loop>
          <source src="images/surrounddepth.gif" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/surrounddepth.gif' width=110%>
        </div>
        <script type="text/javascript">
          function LowRankOcc_start() {
            document.getElementById('LowRankOcc').style.opacity = "1";
          }

          function LowRankOcc_stop() {
            document.getElementById('LowRankOcc').style.opacity = "0";
          }
          LowRankOcc_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:top;text-align: justify">
        <span class="papertitle">SurroundDepth: Entangling Surrounding Views for Self-Supervised Multi-Camera Depth Estimation </span>  
        <br>      
        <span class="author"><a href="https://weiyithu.github.io/"> Yi Wei*</a>, <strong>Linqing Zhao</strong>*, <a href="https://wzzheng.net/"> Wenzhao Zheng</a>, 
                        <a href="https://scholar.google.com/citations?user=NmwjI0AAAAAJ&hl=zh-CN&oi=sra"> Zheng Zhu</a>, 
                        <a href="https://raoyongming.github.io/"> Yongming Rao</a>, 
              Guan Huang,
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu</a>#,
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a> </span> 
        <br> 
        <span class="publications"><em>Conference on Robot Learning (<strong>CoRL</strong>), 2022 </em></span>
        <br> 
        <p>
          We propose a SurroundDepth method to incorporate the information from multiple surrounding views to predict depth maps across cameras.
        </p> 
      </td>
    </tr>


                <tr onmouseout="LowRankOcc_stop()" onmouseover="LowRankOcc_start()">
      <td style="padding:20px;width:25%;vertical-align:top;text-align: justify">
        <div class="one">
          <div class="two" id='LowRankOcc'><video  width=100% muted autoplay loop>
          <source src="images/HSA.png" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/HSA.png' width=110%>
        </div>
        <script type="text/javascript">
          function LowRankOcc_start() {
            document.getElementById('LowRankOcc').style.opacity = "1";
          }

          function LowRankOcc_stop() {
            document.getElementById('LowRankOcc').style.opacity = "0";
          }
          LowRankOcc_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:top;text-align: justify">
        <span class="papertitle">Learning Hybrid Semantic Affinity for Point Cloud Segmentation </span>  
        <br>      
        <span class="author">
              Zhanjie Song, 
              <strong>Linqing Zhao</strong>, 
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou</a>#
        <br> 
        <span class="publications"><em>IEEE Transactions on Circuits and Systems for Video Technology (<strong>TCSVT</strong>), 2021. </em></span>
        <br> 
        <p>
          We present a hybrid semantic affinity learning method (HSA) to capture and leverage the dependencies of categories for 3D semantic segmentation, which aims to learn the label dependencies between 3D points from a hybrid perspective.
        </p> 
      </td>
    </tr>




	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <h2>Honors</h2>
            <p>
              <li style="margin: 5px;"> 
                <b>Shuimu Scholar, Tsinghua University:</b> 2024
              </li>
              <li style="margin: 5px;"> 
                <b>Academic Scholarship of Tianjin University:</b> 2017, 2019, 2022
              </li>
              <li style="margin: 5px;"> 
                <b>First Prize of the Doctoral Student Academic Forum of the School of EIE, Tianjin University:</b> 2022
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>
            

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <h2>Academic Services</h2>
            <p>
              <li style="margin: 5px;"> 
                <b>Conference Reviewer:</b> ICRA, IROS, ACM MM, ICME, ICASSP
              </li>
              <li style="margin: 5px;"> 
                <b>Journal Reviewer:</b> TIP, TCSVT, TBIOM, Pattern Recognition
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>
       
  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Website Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>


		  
      </td>
    </tr>
  </table>
 
<p><center>
	      
	  <br>
	    &copy; 
</center></p>
</body>

</html>
